BAYESIAN MODEL COMPaRISON:
> First applied to NN in MacKay(1992);
> Model M with a single parameter w, trainin inputs x and training labels y;
> Bayes Theorem:
    ->describes the probability of an event, based on prior knowledge of conditions that might be related to the event.For example, if the risk of developing health problems 
      is known to increase with age, Bayes' theorem allows the risk to an individual of a known age to be assessed more accurately (by conditioning it on their age) than simply 
      assuming that the individual is typical of the population as a whole.
    ->one of the many applications of Bayes' theorem is Bayesian inference, a particular approach to statistical inference. when applied, the probabillities involved in the theorem
      may have different probabillity interpretations. with Bayesian probabillity interpretation, the theorem express how a degree of belief, expressed as a probabillity, should 
      rationally change to account for the availability of related evidence. Bayesian inference is fundamental to Bayesian statistics.
    ->statement of theorem: Bayes' theorem is stated mathematically as the following equation:

                            P(A|B) = P(B|A)P(A) / P(B)
      where A and B are events and P(B)!=0
      > P(A|B) is a conditional probability: the probability of event A occurring given that B is true. It is also called the posterior probability of A given B;
      > P(B|A) is also a conditional probability: the probability of event B occurring given that A is true. It can also be interpreted as the likelihood
        (https://en.wikipedia.org/wiki/Likelihood_function) of A given a fixed B because P(B|A) = L(A|B);
      > P(A) and P(B) are the probabillities of observig A and b respectively without any given conditions. They are known as the marginal probability or prior probability
        (https://en.wikipedia.org/wiki/Prior_probability);
      > A and B must be different eventes;

    ->application:

                            P(w|y,x;M) = P(y|w,x;M)P(w;M) / P(y|x;M) !!!!!!!!!!!!!!!!!!!!!!!!TODO READ!!!!

      ***

    ->MacKay:
      >Basic framework for learning in networks: The training set for the mapping to be learned is a set of input-target pairs D={x^m,t^m}, where m is a label running over the pairs. 
        A neural network architecture A is invented, consisting of a specification of the number of layers, the number of units in each layer, the type of activation function performed 
        by each unit, and the available connections between the units. If a set of values w is assigned to the connections in the network, the network defines a mapping y(x;w,A) from
        the input activities x to the output activities y. The distance of this mapping to the training set is measured by some error funtion (E_D is defined as the error in the 
        entire data set );
      >The task of "learning" is to find a set of connections w that gives a mapping that fits the trainingset well, that is, has small E_D; it is also hoped that the learned 
        connections will "generalize" well to new examples.
      >Plain backpropagation learns by performing gradient descent on E_D in w-space. Modifications include the addition of a "momentum" term, and the inclusion of noise in the 
        descent process;
      >One popular way of comparing networks trained with different parameter values is to assess their performance by measuring the error on an unseen test set or by similar 
        cross-validation techniques. The data are divided into two sets, a training set that is used to optimize control parameters w of the network, and a test set that is used to 
        optimize control parameters such as alpha and the architecture A.
      >The utility of these techniques in determining values for the the parameters alpha and betha or for comparing alternative network solutions, etc., is limited because a large
        test set may be needed to reduce the signal-to-noise ratio in the test error, and cross-validation is computationally demanding.
      >Probabilistic view of learning that is an important step toward solving the problems listed above. The idea is to force a probablistic interpretation onto the neural network
        technique so as to be able to make objective statements. This interpretation does not involve the addition of any new arbitrary functions or parameters, but it involves 
        assigning a meaning to the functions and parameters that are already used. 
      >Review the probabilistic interpretation of network learning:

                ->Likelihood: A network with specified architecture A and connections w is viewed as making predictions abaout the target outputs as a function of input x in 
                  accordance with the probability distribution:


                                P(t^m|x^m,w,betha,A) = exp[-betha.E(t^m|x^m,w,a)] / Z_m(betha)

                  where Z_m(betha) = int(dt exp(-betha.E)). E is the error for a single datum, and betha is a measure of the presumed noise included in t. If E is the quadratic error 
                  function then this corresponds to the assumption that t includes additive gaussian noise with variance sigma_{niu}^2 = 1/betha.
                
                ->Prior: A prior probability is assigned to alternative network connection strengths w, written in the form:

                                P(w|aloha,A,R) = exp[-alpha.E_w(w|A)] / Z_w(alpha)

                  where Z_w = int(d^kw exp(-alpha.E_w)). Here alpha is a measure of the characteristic expected connection magnitude. If E_w is quadratic then weights are expected 
                  to come from a gaussian with zero mean and variance sigma_w^2 = 1/alpha. Alternative "regulizers" R (each using a different energy function E_w) implicitly 
                  correspond to alternative hypotheses about the statistics of the environment.

                ->The posterior probability of the network connections w is then

                                P(w|D,alpha,betha,A,R) = exp(-alpha.E_w-betha.E_D) / Z_M(alpha,betha)

                  where Z_M(alpha,betha) = int(d^kw exp(-alpha.E_w-betha.E_D)).The exponent in this expression is the same as (minus) the objective function M.

      >Under this framework, minimization of M=alpha.E_w + betha.E_D is identical to finding the (locally) most probable parameters w_{MP};minimization of E_D aloneis identical to
        finding the maximum likelihood parameters w_{ML}. Thus an interpretation has been given to backpropagation's energy functions E_D and E_w, and to the parameters alpha and 
        betha. It should be emphasized that "the probability if the connections w" is a measure of plausibility that the model's parameters should have a specified value w; this has
        nothing to do with the probability that a particular algorithm might converge to w.

      >Determination of alpha and betha. By Bayes' rule, the posterior probability for these parameters is

                                P(alpha, betha|D,A,R) = P(D|alpha,betha,A,R)P(alpha,betha) / P(D|A,R)

        Now if we assign a uniform prior to (alpha,betha), the quantity of interest for assigning preferences to (alpha,betha) is the first term on the right-hand side, the evidence
        for alpha and betha, which can be written as

                                P(D|alpha,betha,A,R) = Z_M(alpha,betha) / (Z_W(alpha).Z_D(betha))

        where Z_M and Z_W were defined earlier and Z_D=int(d^ND.exp(-betha.E_D))


      ***
      >The likelihood, P(y|w,x;M) = Prod_i(y_i|w,x_i;M) = exp(-H(w;M)), where H(w,M) = -Sum_i(ln(P(y_i|w,x_i;M))) denotes the cross-entropy of unique categorical labels. We 
        typically use a Gaussian prior, P(w;M) = sqrt(lambda/(2.pi)).exp(-lambda.w^2 / 2), and therefor the posterior probability density if the parameter given the training data,
        P(w|y,x;M) aprox sqrt(lambda / (2.pi)).exp(-C(w;M)), where C(w;M) = H(w;M)+lambda.w^2/2 denotes L2 regularized cross entropy, or "cost function", and lambda is the 
        regularization coefficient. The value w_0 which minimizes the cost function lies at the maximum of this posterior. To predict an unknown label y_t of a new input x_t, we
        should compute the integral,

                                P(y_t|x_t,y,x;M) = int(dw P(y_t|w,x_t;M).P(w|y,x;M))

                                                 = int(dw P(y_t|w,x_t;M).exp(-C(w;M))) / int(dw exp(-C(w;M))) 


      >However these integrals are dominated by the region near w_0, and since P(y_t|w,x_t;M) is smooth we usually approximate P(y_t|x_t,x,y;M) aprox/ P(y_t|w_0,x_t;M). 
      Having minimized C(w;M) to find w_0, we now wish to compare two different models and select the best one. The probability ratio,

                                P(M_1|y,x) / P(M_2|y,x) = P(y|x;M_1) / P(y|x;M_2) . P(M_1) / P(M_2)

      >The second factor on the right is the prior ratio, which describes which model is most plausible . To avoid unnecessary subjectivity, we usually set this to 1. Meanwhile the
      first factor ont the right is the evidence ratio, which controls how much the training data changes our prior beliefs.
      

    ->Bayes Theorem and Generalization:

      >Zhang et al. (2016) showed that deep neural networks generalize well on training inputs with informative labels, and yet the same model can drastically overfit on the same 
        input images when the labels are randomized:

          LER!!!!!!!!!!!!!!

      >To demonstrate that these observations are not unique to deep networks, let's consider a far simpler model, logistic model. We form a small balanced training set comprising 
        800 images from MNIST, of which half have true label "0" and half have true label "1". Our test set is also balanced, comprising 5000 MNIST images if zeros and 5000 MNIST
        images of ones. There are two tasks. In the first task, the labels of both the training and test sets are randomized. In the second task, the labels are informative, 
        matching the true MNIST labels. Since the images contain 784 pixels, our model has just 784 weights and 1 bias.
      >When trained on informative labels, the model generalizes well to test set, so long as it is weakly regularized. However the model also perfectly memorizes the random labels, 
        replicating the observations of Zhang et al. (2016) in deep neural networks. No significant improvement in model performance is observed as the regularization coefficient
        increases.
      >Bayesian model comparison has explained our resilts in a logistic regression. We conclude that Bayesian model comparison is quantitatively consistent with the results of 
        Zhang et al. (2016) in linear models where we can compute the evidence, and quantitatively with their results in deep networks where we cannot


    ->Bayes Theorem and Stochastic Gradient Descent:

      >We showed above that generalization is strongly correlated with the Bayesian evidence, a weighted combination of the depth of a minimum (the cost function) and its breadth
        (the Occam factor). Consequently Bayesians often add isotropic Gaussian noise to the gradient (Welling & Teh, 2011).
      >In appendix A, we show this drives the parameters towards broad minima whose evidence is large. The noise introduced by small batch training is not isotropic, and its 
        covariance matrix is a functionof the parameters values, but empirically Keskar et al. (2016) found it has similar effects, driving the SGD away from sharp minima. This 
        paper therefore proposes Bayesian principles also account for the "generalization gap", whereby the test set accuracy often falls as the SGD towards deep minima, while 
        noise drives SGD towards broad minima, we expect the test set performance to show a peak at an optimal batch size, which balances these competing contributions to the evidence.

            ->Keskar et al. (2016):

              LER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

            ->Appendix A Bayesian Posterior Sampling And Langevin Dynamics:

              LER!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!

      >We were unable to observe a generalization gap in linear models (since linear models are convex there are no sharp minima to avoid). Instead we consider a shallow neural network
        with 800 hidden units and RELU hidden activations, trained on MNIST without regularization. We use SGD with a momentum parameter of 0.9. Unless otherwise stated, we use a 
        constant learning rate of 1.0 which does not depend on the batch size or decay during training. Furthermore, we train on just 100 images, selected at random from the MNIST 
        training set.This enables us to compare small batch to full batch training. We emphasize that we are not trying to achieve optimal performance, but to study a simple model 
        which shows a generalization gap between small and large training.
      >Focus on the test set accuracy (since this converges as the number of gradient updates increases). In figure XXXX, we exhibit training curves for a range of batch sizes 
        between 1 and 1000. We find that the model cannoit train when the batch size B <=aprox 10. In figure XXXX we plot the mean test set accuracy after 10000 training steps.
        A clear peak emerges, indicating that there is indeed an optimum batch size which maximizes the test accuracy, consistent with Bayesian intuition. The results of 
        Keskar et al. (2016) focused on the decay in test accuracy above this optimum batch size.

    ->We showed above that the test accuracy peaks at an optimal batch size, if one holds the other SGD hyper-parameter constant. We argued that this peak arises from the tradeoff 
      between depth and breadth in the Bayesian evidence. However it is not the batch size itself which controls this tradeoff, but the underlying scale of random flutuations in the SGD
      dynamics. We now identify this SGD "noise scale", and use it to derivate three scaling rules which predict how the optimal batch size depends on the learning rate, training set size
      and momentum coefficient. A gradient update,

                                delta w = -epsilon/N(dC/dw + (dC'/dw - dC/dw))

      where epsilon is the learning rate, N the training set size, dC/dw = sum_{i=1}^N(dC_i/dw) the true gradient, and dC'/dw = N/B . sum_{i=1}^N(dC_i/dw) the estimated gradient evaluated 
      on a mini-batch. 
    >The expected gradient of a single example, <dC_i/dw> = 1/N . dC/dw, while <dC_i/dw . dC_j/dw> = (1/N . dC/dw)^2 + F(w) . dirac_{ij}. F(w) is a matrix describing the theorem and model 
      the gradient error alpha = (dC'/dw - dC/dw) with Gaussian random noise (We discuss this aprocimation briefly in appondix C) 
      ...

      ->Appendix C:
        >
